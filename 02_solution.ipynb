{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nangs.pde import PDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solutions\n",
    "\n",
    "> This module contains the different function approximators available to solve PDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Solution(nn.Module):\n",
    "    \"Currently, only MLPs are supported as solution approximators wiht same number of neurons and activation \\\n",
    "    function per layer\"\n",
    "    def __init__(self, inputs, outputs, layers, neurons, activations):\n",
    "        super().__init__()\n",
    "        \n",
    "        # checks\n",
    "        if not isinstance(inputs, int) or inputs <= 0: raise Exception('inputs must be a postive integer')\n",
    "        if not isinstance(outputs, int) or outputs <= 0: raise Exception('outputs must be a positive integer')\n",
    "        if not isinstance(layers, int) or layers <= 0: raise Exception('layers must be a positive integer')\n",
    "        if not isinstance(neurons, int) or neurons <= 0: raise Exception('neurons must be a positive integer')\n",
    "        if not isinstance(activations, str): raise Exception('activation must be a string')\n",
    "\n",
    "        # activaton function\n",
    "        self.activation = activations\n",
    "        # layers\n",
    "        self.fc_in = block(inputs, neurons, self.activation)\n",
    "        self.fc_hidden = nn.ModuleList()\n",
    "        for layer in range(layers):\n",
    "            self.fc_hidden.append(block(neurons, neurons, self.activation))\n",
    "        self.fc_out = nn.Linear(neurons, outputs)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        for layer in self.fc_hidden:\n",
    "            x = layer(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def getActivation(a):\n",
    "    if a == 'relu': return nn.ReLU(inplace=True)\n",
    "    elif a == 'sigmoid': return nn.Sigmoid(inplace=True)\n",
    "    else: raise Exception(f'activation function {a} not valid')\n",
    "\n",
    "def block(i, o, a):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(i, o), \n",
    "        #nn.BatchNorm1d(o),\n",
    "        getActivation(a)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Solution(\n",
       "  (fc_in): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_hidden): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pde = PDE(inputs=['x', 't'], outputs=['p'])\n",
    "\n",
    "# add values and bocos\n",
    "\n",
    "mlp = {'layers': 3, 'neurons': 100, 'activations': 'relu'}\n",
    "pde.buildSolution(mlp)\n",
    "\n",
    "pde.solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1038],\n",
       "        [-0.1605]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = torch.tensor([[1, 2], [3, 4]]).float()\n",
    "test_output = pde.solution(test_input)\n",
    "test_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
